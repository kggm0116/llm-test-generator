# -*- coding: utf-8 -*-
"""llm-test-generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aHlfi1NbNWwp89--gmL9XU8sxl3OsJuh
"""

# Commented out IPython magic to ensure Python compatibility.
# Установка библиотек, при по необходимости

try:
  import pymorphy2
except:
#   %pip install pymorphy2

try:
  import openai
except:
#   %pip install openai

# Настройки окружения в колабе

from os import mkdir
from os import path

DIR_ROOT = "/content"
DIR_RESOURCES = DIR_ROOT + "/resources"
DIR_TESTS = DIR_ROOT + "/tests"
DIR_UPLOADS = DIR_ROOT + "/uploads"
DIR_LOGS = DIR_ROOT + "/logs"
DIR_ARCHIVES = DIR_ROOT + "/archives"
DIR_REVIEWS = DIR_ROOT + "/reviews"

def create_folder(folder, then = None):
  if not path.exists(folder):
    mkdir(folder)
    if then != None: then()

def show_add_class_lists_message():
  raise ValueError(f"Загрузите списки слов по классам и словарь Шарова в папку '{DIR_RESOURCES}', затем перезапустите программу.")

create_folder(
    DIR_RESOURCES,
    then = show_add_class_lists_message
)
create_folder(DIR_UPLOADS)
create_folder(DIR_LOGS)
create_folder(DIR_TESTS)
create_folder(DIR_ARCHIVES)
create_folder(DIR_REVIEWS)

# Файлы в колабе

import os
from google.colab import files

def list_files(folder):
  return [f"{folder}/{file}" for file in os.listdir(folder) if not ".ipynb" in file]

def save_file(file_name, content):
  with open(file_name, 'w') as file:
    file.write(content)

def read_file(file_name, encoding = None):
  with open(file_name, 'r', encoding = encoding) as file:
    return file.read()

def download_file(file_name):
  files.download(file_name)

def delete_file(file_name):
  os.remove(file_name)

def clear_folder(folder):
  for f in list_files(folder):
    delete_file(f"{folder}/{f}")

def upload_to_folder(folder):
  os.chdir(folder)
  uploads = files.upload()
  os.chdir(DIR_ROOT)
  return uploads

# API

# Документация по моделям GPT: https://platform.openai.com/docs/models

from openai import OpenAI

def make_console_request(message: str):
  print("\nЗапрос:")
  print(message)
  response = input("Ответ: ")
  return (response, "Получен через консоль")

openai_client = OpenAI(
    api_key = "",
    base_url = "https://api.proxyapi.ru/openai/v1"
)

openai_role = """
  Ты - ассистент для создания тестов на русском языке для школьников.
"""

log_responses = True # Для отладки на случай падения программы

def make_openai_proxy_request(message: str, model: str):
  response = openai_client.chat.completions.create(
      model=model,
      messages=[
          { "role": "system", "content": openai_role },
          { "role": "user", "content": message },
      ]
  )
  if log_responses:
    print(f"Response:\n{response.choices[0].message.content}")
  return (response.choices[0].message.content, str(response))

request_modes = [
    "console",
    "gpt3",
    "gpt4",
]

request_mode = "undefined"

def make_request(message: str):
  if (request_mode == "console"):
    return make_console_request(message)
  elif (request_mode == "gpt3"):
    return make_openai_proxy_request(message, "gpt-3.5-turbo")
  elif (request_mode == "gpt4"):
    return make_openai_proxy_request(message, "gpt-4-turbo")
  else: raise ValueError(f"Неизвестный режим запроса: {request_mode}")

# Общие функции

from google.colab import files
from itertools import chain
import os

def flatten(nested_list):
  return list(chain.from_iterable(nested_list))

def strings_equal(str1, str2):
  return str1.lower().strip() == str2.lower().strip()

def make_yes_no_prompt(message):
  while True:
    response = input(f"{message} (y/n) ")
    if response == "y":
      return True
    elif response == "n":
      return False

# Разделение списка на части длиной <= size
def windowed(items, size):
  return [
      items[i * size:(i + 1) * size]
      for i in range((len(items) + size - 1) // size)
  ]

# Разделение списка на count частей
def chunked(items, count):
    k, m = divmod(len(items), count)
    return list((items[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(count)))

# Для парсинга

import datetime
import json

def serialize_sets(obj):
  if isinstance(obj, set):
      return list(obj)
  return obj

def format_text_for_logging(text):
  return text.replace("\n", " ").strip()

def format_datetime(datetime):
  return datetime.strftime("%Y.%m.%d %H-%M-%S")

def encode_log_data(log_data):
  return json.dumps(
      log_data,
      ensure_ascii = False,
      indent=2,
      default = serialize_sets
  )

# Получения слов из относительных списков классов (в формате словаря "класс : слова")

import pandas as pd

word_classes = range(2, 8)

relative_dictionaries = [
    pd.read_csv(f"{DIR_RESOURCES}/{c} класс - относительный.csv")
    for c in word_classes
]
relative_dictionary_data = [
    zip(
        csv["Слово"].tolist(),
        csv["Сумма_слов"].tolist(),
    )
    for csv in relative_dictionaries
]

frequency_dictionaries = [
    pd.read_csv(f"{DIR_RESOURCES}/{c} класс - частотный.csv")
    for c in word_classes
]

frequency_dictionary_data = [
    zip(
        csv["Слово"].tolist(),
        csv["Сумма_слов"].tolist(),
        csv["Нормализованная_частотность"].tolist(),
    )
    for csv in frequency_dictionaries
]

# {класс: [слово]}
relative_word_lists = dict(zip(
    word_classes, [
      {
          word: count if type(count) == int else int(float(count.replace(",", ".")))
          for word, count in data
      }
      for class_index, data in enumerate(relative_dictionary_data)
    ]
))

# {класс: {слово: (сумма, частота)}}
frequency_word_lists = dict(zip(
    word_classes, [
      {
          word: (count if type(count) == int else int(float(count.replace(",", "."))), frequency)
          for word, count, frequency in data
      }
      for class_index, data in enumerate(frequency_dictionary_data)
    ]
))

def get_words_of_class(word_class):
  return set(relative_word_lists[word_class].keys())

def get_word_count_in_class(word, word_class):
  return relative_word_lists[word_class].get(word, 0)

def get_word_count_until_class(word, word_class):
  classes = range(2, word_class + 1)
  data_by_class = [frequency_word_lists[c].get(word, (0, 0)) for c in classes]
  counts = [c for c, _ in data_by_class]
  return sum(counts)

def get_word_class(word):
  word_class = None
  for class_number, words in relative_word_lists.items():
    if word in words.keys():
      word_class = class_number
  return word_class

# Запросы к модели

from textwrap import dedent

def get_distractors_prompt(word, word_pos, count):

  return dedent(f"""
    Найди {count} гиперонимов или гипонимов слова '{word}'.
    Они должны состоять из одного слова.
    Слова не должны быть однокоренными со словом '{word}'.
    Слова должны быть части речи {word_pos}.
    Желательно, чтобы слова соответствовали предметной области слова '{word}', и были схожего уровня сложности.
    Выпиши их в одну строчку через запятую, без нумерации и дополнительных пояснений.
  """)

def get_definition_prompt(word, pos, distractors):
  distractor_list = ', '.join([f"'{d}'" for d in distractors])
  unacceptable_words = ', '.join([f"'{w}'" for w in [word] + list(distractors)])
  return dedent(f"""
    Напиши толкование для слова '{word}' части речи {pos}.
    Толкование должно быть кратким и однозначным.
    В толковании не должны использоваться слова {unacceptable_words} и слова однокоренные им.
    Соблюдай в ответе правила русского языка.
    Не добавляй в ответ дополнительных пояснений и стилизацию.
    Напиши только само толкование, без целевого слова.
  """)

def get_correction_prompt(definition, word, wrong_words, distractors):
  distractor_list = ', '.join([f"'{d}'" for d in distractors])
  unacceptable_words = ', '.join([f"'{w}'"  for w in [word] + list(wrong_words)])
  return dedent(f"""
    Исправь толкование '{definition}' для слова '{word}' так, чтобы в нем не использовались слова {unacceptable_words} и слова однокоренные им.
    Толкование должно сохранить исходный смысл.
    Толкование должно быть кратким и однозначным.
    Соблюдай в ответе правила русского языка.
    Не добавляй в ответ дополнительных пояснений и стилизацию.
    Напиши только само толкование, без определяемого слова.
  """)

# Функции по работе с текстом/языком

import pymorphy2
import nltk
import string
from nltk.stem.snowball import SnowballStemmer

nltk.download("punkt")
nltk.download("stopwords")

punctuation_marks = string.punctuation

morph = pymorphy2.MorphAnalyzer()
stemmer = SnowballStemmer("russian")

letter_replacements = {
  'а́': 'а',
  'е́': 'е',
  'и́': 'и',
  'о́': 'о',
  'у́': 'у',
  'ы́': 'ы',
  'э́': 'э',
  'ю́': 'ю',
  'я́': 'я',
  "ʻ": "",
}

# Похоже, стеммер не работает с буквой 'ё'.
# Если заменять на 'е', то вроде работает нормально, но могут быть исключения.
def prepare_token(token):
  return token.replace('ё', 'е')

def get_lemma(word, expected_pos = None):
  variants = morph.parse(prepare_token(word))
  matching = variants if expected_pos == None else [
      variant for variant in variants
      if variant.tag.POS == expected_pos
  ]
  selected = None if len(matching) == 0 else matching[0]
  if selected == None:
    return None
  else:
    result = selected.normal_form
    for replaced, replacement in letter_replacements.items():
      result = result.replace(replaced, replacement)
    return result

def get_pos_variants(word):
  variants = morph.parse(prepare_token(word))
  return set([variant.tag.POS for variant in variants])

def get_best_pos_variant(word):
  return morph.parse(prepare_token(word))[0].tag.POS

# NLTK - константы

# https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html

# Части речи, которые не должны иметь целевые слова и дистракторы.
# Допустимые части речи - по сути, глаголы и существительные.
# Прилагательные и причастия было решено исключить,
# поскольку по ним трудно составить нормальные толкования.
allowed_defined_word_pos = set([
    "VERB",
    "INFN",
    "NOUN"
])

# Части речи, которые всегда считаются валидными в составе толкования
# (т.е. такие, которые точно будут знакомы, даже если их нет в словарях)
ignored_definition_word_pos = set([
    "NUMR", # числительное,
    "NPRO", # местоимение-существительное
    "PREP", "CONJ", "PRCL", # союзы, предлоги, частицы
    "INTJ" # междометие
])

# Теги, которые не должны иметь целевые слова и дистракторы.
# Надо иметь в виду, что теги проставляются не слишком надежно.
excluded_defined_word_tags = set([
    "Abbr", # аббревиатура
    "NUMB", # число
])

# Удаление лишних слов и получение лемм из определения

import nltk
import string

try:
  from nltk.corpus import stopwords
except:
  nltk.download('stopwords')
  from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Код для просмотра слов из словаря Шарова, сгруппированных по части речи из NLTK:
# words = {w: get_best_pos_variant(w) for w in sharov_dictionary.keys()}
# grouped = {pos: [w for w, p in words.items() if p == pos] for pos in set(words.values()) if pos == "ADVB"}
# print(*grouped.items(), sep = "\n")

def get_definition_lemmas(word, definition):
  initial_tokens = word_tokenize(definition.strip("."))

  # Удалим из рассмотрения некоторые токены

  stop_words = stopwords.words('russian')

  tokens_to_remove = set([
      token for index, token in enumerate(initial_tokens)
      if (token in stop_words + [*punctuation_marks]
          or get_best_pos_variant(token) in ignored_definition_word_pos
          or index == len(initial_tokens) - 1 and strings_equal(token, word)
          or index == len(initial_tokens) - 2 and token == "-")
  ])
  final_tokens = [
      token for token in initial_tokens
      if not token in tokens_to_remove
  ]

  lemmas = [get_lemma(token) for token in final_tokens]

  log_data = {
      "initial_tokens": initial_tokens,
      "removed_tokens": tokens_to_remove,
      "final_tokens": final_tokens,
      "lemmas": lemmas,
  }

  return (lemmas, log_data)

# Классифицировать леммы (не найденные в списках классов) по словарю Шарова

import pandas as pd

sharov_csv = pd.read_csv(f"{DIR_RESOURCES}/Частотный словарь Шарова v3.csv")
sharov_dictionary = dict(zip(
    list(sharov_csv["Lemma"]),
    list(sharov_csv["Freq(ipm)"])
))

# Классифицировать леммы по принадлежности к классам (ниже, выше или не определен)

def classify_lemmas(word_class, lemmas):
  low_classes = list(range(2, word_class))
  high_classes = list(range(word_class + 1, 8))
  all_low_class_words = flatten([get_words_of_class(c) for c in low_classes])
  all_same_class_words = get_words_of_class(word_class)
  all_high_class_words = flatten([get_words_of_class(c) for c in high_classes])
  low_class_words = set(all_low_class_words) & set(lemmas)
  same_class_words = set(all_same_class_words) & set(lemmas)
  high_class_words = set(all_high_class_words) & set(lemmas)
  unrecognized_words = set(lemmas) - low_class_words - high_class_words - same_class_words
  words_in_sharov = [w for w in unrecognized_words if w.lowercase() in sharov_dictionary.keys()]
  words_not_in_sharov = set(unrecognized_words) - set(words_in_sharov)
  return (low_class_words,
          same_class_words,
          high_class_words,
          words_in_sharov,
          words_not_in_sharov)

# Отбор целевых слов для составления вопросов

import nltk
import pymorphy2

defined_word_min_count_in_class = 5
defined_word_max_count_in_class = 200

distractor_min_count_in_classes = 5
distractor_max_count_in_classes = 200
distractor_min_frequency_in_sharov = 2
distractor_max_frequency_in_sharov = 75

definition_word_min_frequency_in_sharov = 2

morph = pymorphy2.MorphAnalyzer()

def is_valid_defined_word(word, expected_word_class):
  word_class = get_word_class(word)

  is_valid_class = word_class == expected_word_class
  pos = get_best_pos_variant(word)
  is_allowed_pos = pos in allowed_defined_word_pos

  has_hyphen = "-" in word

  is_capitalized = word[0].isupper()

  nltk_tags = morph.parse(word)[0].tag
  excluded_tags = [
      tag for tag in excluded_defined_word_tags
      if tag in nltk_tags
  ]

  count_in_class = get_word_count_in_class(word, word_class)
  has_valid_count_in_class = defined_word_min_count_in_class <= count_in_class <= defined_word_max_count_in_class

  if not is_allowed_pos:
    return (False, f"Не подходит по части речи ({pos})")
  elif is_capitalized:
    return (False, "Начинается с заглавной буквы")
  elif not is_valid_class:
    return (False, f"Неверный класс ({word_class})")
  elif len(excluded_tags) != 0:
    return (False, f"Содержит нежелательные NLTK-теги ({', '.join(excluded_tags)})")
  elif not has_valid_count_in_class:
    return (False, f"Недопустимая встречаемость в словаре по классу ({count_in_class})")
  elif has_hyphen:
    return (False, "Содержит дефис")
  else:
    return (True, "Подходит по всем критериям")

def is_valid_distractor(word, defined_word):
  defined_word_class = get_word_class(defined_word)
  word_class = get_word_class(word)
  pos = get_best_pos_variant(word)
  is_matching_pos = pos == get_best_pos_variant(defined_word)

  has_hyphen = "-" in word

  is_capitalized = word[0].isupper()

  nltk_tags = morph.parse(word)[0].tag
  excluded_tags = [
      tag for tag in excluded_defined_word_tags
      if tag in nltk_tags
  ]

  is_high_class = word_class != None and word_class >= defined_word_class
  count_in_previous_classes = get_word_count_until_class(word, defined_word_class - 1)
  has_valid_count_in_previous_classes = distractor_min_count_in_classes <= count_in_previous_classes <= distractor_max_count_in_classes

  sharov_frequency = sharov_dictionary.get(word, 0)
  is_valid_sharov_word = distractor_min_frequency_in_sharov <= sharov_frequency <= distractor_max_frequency_in_sharov

  if not is_matching_pos:
    return (False, f"Не совпадает по части речи ({pos})")
  elif is_capitalized:
    return (False, "Начинается с заглавной буквы")
  elif len(excluded_tags) != 0:
    return (False, f"Содержит нежелательные NLTK-теги ({', '.join(excluded_tags)})")
  elif is_high_class:
    return (False, f"Слишком высокий класс ({word_class})")
  elif count_in_previous_classes != 0 and not has_valid_count_in_previous_classes:
    return (False, f"Недопустимая встречаемость в словарях по классам ({count_in_previous_classes})")
  elif sharov_frequency != 0 and not is_valid_sharov_word:
    return (False, f"Недопустимая частотность в словаре Шарова ({sharov_frequency})")
  elif count_in_previous_classes == 0 and sharov_frequency == 0:
    return (False, f"Не встречается в списках по классам и словаре Шарова")
  elif has_hyphen:
    return (False, "Содержит дефис")
  elif has_valid_count_in_previous_classes:
    return (True, f"Допустимая встречаемость в словарях по классам ({count_in_previous_classes})")
  elif is_valid_sharov_word:
    return (True, f"Допустимая частотность в словаре Шарова ({sharov_frequency})")
  else:
    return (False, f"Непредвиденное состояние системы")

def is_valid_definition_word(word, defined_word_class):

  word_class = get_word_class(word)
  is_low_class = word_class != None and word_class < defined_word_class
  is_high_class = word_class != None and word_class >= defined_word_class

  sharov_frequency = sharov_dictionary.get(word, 0)
  is_valid_sharov_word = sharov_frequency != 0 and definition_word_min_frequency_in_sharov <= sharov_frequency

  if is_high_class:
    return (False, f"Высокого класса ({word_class})")
  elif not is_low_class and sharov_frequency != 0 and not is_valid_sharov_word:
    return (False, f"Недопустимая частотность в словаре Шарова ({sharov_frequency})")
  elif word_class == None and sharov_frequency == 0:
    return (False, f"Не встречается в списках по классам и словаре Шарова")
  elif is_low_class:
    return (True, f"Низкого класса ({word_class})")
  elif is_valid_sharov_word:
    return (True, f"Допустимая частотность в словаре Шарова ({sharov_frequency})")
  else:
    return (False, f"Непредвиденное состояние системы")

import time
import random
import math

def get_random_words_with_different_frequencies(
    word_class,
    count,
    rng_seed
):
  rng = random.Random()
  rng.seed(rng_seed if rng_seed != None else time.time_ns())
  valid_words = [
      word for word in get_words_of_class(word_class)
      if is_valid_defined_word(word, word_class)[0]
  ]
  words = sorted(
      [
          (w, frequency_word_lists[word_class].get(w, (-1, -1)))
          for w in valid_words
      ],
      key = lambda x: x[1][0] # Сортировка по возрастанию встречаемости
  )
  min_count = words[0][1][0]
  max_count = words[-1][1][0]
  count_window_size = (max_count - min_count) / count
  count_ranges = [
      (min_count + i * count_window_size, min_count + (i + 1) * count_window_size)
      for i in range(0, count)
  ]
  words_by_range = [
      [w for w in words if r[0] <= w[1][0] < r[1]]
      for r in count_ranges
  ]
  for i in reversed(range(0, count)):
    if len(words_by_range[i]) == 0:
      previous_non_empty_range_index = next(
          (j for j in reversed(range(0, i)) if len(words_by_range[j]) > 0),
          None
      )
      if previous_non_empty_range_index == None: continue
      half = math.floor(len(words_by_range[previous_non_empty_range_index]) / 2.0)
      words_by_range[i] = words_by_range[previous_non_empty_range_index][half:]
      words_by_range[previous_non_empty_range_index] = words_by_range[previous_non_empty_range_index][:half]
  random_words = [rng.choice(w) for w in words_by_range]
  return random_words

# for w in get_words_of_class(6):
#   print(f"{w}: {is_valid_word(w, 6)}")
# len(get_valid_words(6))
# get_random_words_with_different_frequencies(
#     word_class = 6,
#     count = 10,
#     rng_seed = 1
# )

# Выбор слов-дистракторов

import string

def get_distractors(word, word_pos, count):
  if (word_pos == None):
    return (None, {})
  prompt = get_distractors_prompt(word, word_pos, count)
  (response, response_details) = make_request(prompt)

  distractors = [
      w.strip().strip(punctuation_marks)
      for w in response.split(",")
  ]
  lemmas_to_distractor = {
      get_lemma(distractor): distractor
      for distractor in distractors
  }

  lemma_validity = {
      lemma: (False, "Является исходным словом") if strings_equal(distractor, word)
      else is_valid_distractor(lemma, word)
      for lemma, distractor in lemmas_to_distractor.items()
  }

  distractor_analysis = [
      (lemmas_to_distractor[lemma], lemma, is_valid, reason)
      for lemma, (is_valid, reason) in lemma_validity.items()
  ]

  valid_distractors = [
      distractor
      for distractor, _, is_valid, _ in distractor_analysis
      if is_valid
  ]

  log_data = {
    "prompt": format_text_for_logging(prompt),
    "response": format_text_for_logging(response),
    "response_details": format_text_for_logging(response_details),
    "distractor_analysis": [
        {
            "distractor": distractor,
            "lemma": lemma,
            "is_valid": is_valid,
            "reason": reason,
        } for distractor, lemma, is_valid, reason in distractor_analysis
    ],
    "valid_distractors": valid_distractors,
  }
  return (valid_distractors, log_data)

# Единичная попытка исправить определение

def correct_definition(
    word,
    word_pos,
    word_class,
    definition,
    distractors,
    previously_excluded_words
):
  (lemmas, lemmas_log_data) = get_definition_lemmas(word, definition)

  lemmas_analysis = {
      lemma: is_valid_definition_word(lemma, word_class)
      for lemma in lemmas
  }

  invalid_lemmas = [
      lemma
      for lemma, (is_valid, _) in lemmas_analysis.items()
      if not is_valid
  ]

  distractors_in_definition = set(lemmas) & set(distractors)

  words_to_fix = set(invalid_lemmas) | distractors_in_definition

  log_data = {
      "lemmas": lemmas_log_data,
      "lemmas_analysis": [
          {
              "lemma": lemma,
              "is_valid": is_valid,
              "reason": reason
          }
          for lemma, (is_valid, reason) in lemmas_analysis.items()
      ],
      "distractors_in_definition": distractors_in_definition,
      "words_to_fix": words_to_fix,
      "correction_prompt": None,
  }

  if len(words_to_fix) == 0:
    return (definition, set(), True, log_data)
  else:
    correction_prompt = get_correction_prompt(
        definition,
        word,
        words_to_fix |
        set(previously_excluded_words) |
        set(distractors),
        distractors
    )
    (response, response_details) = make_request(correction_prompt)
    log_data["correction_prompt"] = format_text_for_logging(correction_prompt)
    log_data["response"] = format_text_for_logging(response)
    log_data["response_details"] = format_text_for_logging(response_details)
    return (response, words_to_fix, False, log_data)

# Исправление определения путем нескольких попыток

max_correction_iterations = 4

def process_definition(definition, word, word_pos, word_class, distractors):
  success = False
  current_definition = definition
  iteration = 0
  log_data = {}
  previously_corrected_words = set()
  for iteration in range(1, max_correction_iterations + 1, 1):
    (corrected_definition,
     words_to_fix,
     success,
     correction_log_data,
    ) = correct_definition(
        word,
        word_pos,
        word_class,
        current_definition,
        distractors,
        previously_corrected_words
    )

    previously_corrected_words |= words_to_fix

    log_data[iteration] = {
        "corrected_definition": corrected_definition,
        "correction": correction_log_data,
    }

    current_definition = corrected_definition

    if success: break

  return (current_definition, success, log_data)

# Общая загрузка тестов

# Для загрузки нескольких тестов, лучше их архивировать,
# потому что браузер может не поддерживать параллельную загрузку большого числа файлов.

def create_tests_from_uploaded(create_test):
  clear_folder(DIR_UPLOADS)
  clear_folder(DIR_TESTS)
  uploads = upload_to_folder(DIR_UPLOADS)
  for file, content in uploads.items():
    file_name = f"{DIR_UPLOADS}/{file}"
    create_test(file_name)

def create_tests_from_logs(create_test):
  clear_folder(DIR_TESTS)
  for file in list_files(DIR_LOGS):
    create_test(file)

def download_all_tests():
  archive_name = f"{DIR_ARCHIVES}/Тесты {format_datetime(datetime.datetime.now())}"
  archive = shutil.make_archive(
      base_name = archive_name,
      root_dir = DIR_TESTS,
      format = 'zip',
  )
  files.download(archive)

# Работа с тестами формата "1 определение - 4 слова"

import re

distractor_count = 16
distractor_frequency_threshold = 100

def generate_1_definition_4_words_question(word):
  word_class = None
  pos = get_best_pos_variant(word)
  word_class = get_word_class(word)

  log_data = {
      "word": word,
      "class": word_class,
      "pos": pos,
  }

  if (word_class == None
      or pos == None
      or pos not in allowed_defined_word_pos):
    return log_data

  word_pos = get_best_pos_variant(word)
  log_data["pos"] = word_pos

  (distractors, distractors_log_data) = get_distractors(
      word,
      word_pos,
      distractor_count
  )

  if (distractors == None):
    log_data["distractors_selection"] = distractors_log_data
    return log_data

  selected_distractors = distractors[:3]
  log_data |= {
      "distractors": selected_distractors,
      "distractors_selection": distractors_log_data,
  }

  definition_prompt = get_definition_prompt(word, word_pos, selected_distractors)
  (response, response_details) = make_request(definition_prompt)

  log_data |= {
      "prompt": format_text_for_logging(definition_prompt),
      "response": format_text_for_logging(response),
      "response_details": format_text_for_logging(response_details)
  }

  definition = response.strip().strip(".")
  (final_definition,
   success,
   iterations_data
  ) = process_definition(
      definition,
      word,
      word_pos,
      word_class,
      selected_distractors
  )
  finalized_definition = format_definition_for_1_definition_4_words(final_definition, word)

  log_data |= {
      "initial_definition": format_text_for_logging(definition),
      "final_definition": format_text_for_logging(final_definition),
      "formatted_definition": format_text_for_logging(finalized_definition),
      "success": success,
      "iterations": iterations_data
  }

  return log_data

def format_definition_for_1_definition_4_words(definition, defined_word):
  tokens = definition.strip(".").split(" ")
  without_target_word = [
      token for index, token in enumerate(tokens)
      if len(token) != 0 and not (
          index == len(tokens) - 1 and strings_equal(token, defined_word)
          or index == len(tokens) - 2 and token == "-"
      )
  ]
  return " ".join(without_target_word)

from google.colab import files
import os
from pathlib import Path
import json
import shutil
import datetime

# Создание теста

def get_1_definition_4_words_test_from_file(file):
  with open(file, 'r') as f:
    log = json.load(f)
  definition = log["formatted_definition"]
  words = log["selected_words"]
  words_lines = '\n'.join([
      f'{index + 1}. {d}'
      for index, d in enumerate(words)
  ])
  output = f"{definition} -\n{words_lines}"
  return output

def create_1_definition_4_words_test_from_file(file):
  output = get_1_definition_4_words_test_from_file(file)
  file_name = f"{DIR_TESTS}/Тест {Path(file).stem}.txt"
  save_file(
      file_name = file_name,
      content = output
  )
  return file_name

def create_1_definition_4_words_tests_from_uploaded():
  create_tests_from_uploaded(
      lambda file: create_1_definition_4_words_test_from_file(file)
  )

def create_1_definition_4_words_tests_from_logs():
  create_tests_from_logs(
      lambda file: create_1_definition_4_words_test_from_file(file)
  )

# Основная функция

import datetime
from google.colab import files

def generate_1_definition_4_words_question_for_single_word(
    word,
    count = 1
):
  files = {}
  for _ in range(count):
    start_time = datetime.datetime.now()
    log_data = generate_1_definition_4_words_question(word)
    end_time = datetime.datetime.now()

    log_data["start_time"] = format_datetime(start_time)
    log_data["end_time"] = format_datetime(end_time)
    output = encode_log_data(log_data)

    files |= {f"{DIR_LOGS}/{word} {format_datetime(start_time)}.json": output}

  return files
  should_save = make_yes_no_prompt("Сохранить лог?")
  if should_save:
    for file, output in files.items():
      save_file(file, output)
    should_download_file = make_yes_no_prompt("Скачать файл?")
    # should_create_test = make_yes_no_prompt("Скачать тест?")
    # if should_create_test:
    #   for file in files:
    #     test = create_1_definition_4_words_test_from_file(file)
    #     download_file(test)
    if should_download_file:
      for file in files:
        download_file(file)

def generate_1_definition_4_words_question_for_single_word_interactive(
    count = 1
):
  word = input("Введите слово: ")

  files = generate_1_definition_4_words_question_for_single_word(word, count)

  should_save = make_yes_no_prompt("Сохранить лог?")
  if should_save:
    for file, output in files.items():
      save_file(file, output)
    should_download_file = make_yes_no_prompt("Скачать файл?")
    # should_create_test = make_yes_no_prompt("Скачать тест?")
    # if should_create_test:
    #   for file in files:
    #     test = create_1_definition_4_words_test_from_file(file)
    #     download_file(test)
    if should_download_file:
      for file in files:
        download_file(file)

# Проверка слов на валидность

def analyze_words(word_class):
  word_analysis = sorted(
      [
          (w, is_valid_defined_word(w, word_class))
          for w in list(relative_word_lists[word_class])
      ],
      key = lambda data: data[1][1]
  )

  valid = [
      f"{w}\t{reason}"
      for w, (is_valid, reason) in word_analysis
      if is_valid
  ]

  invalid = [
      f"{w}\t{reason}"
      for w, (is_valid, reason) in word_analysis
      if not is_valid
  ]

  save_file(f"{DIR_ROOT}/valid_words.txt", "\n".join(valid))
  save_file(f"{DIR_ROOT}/invalid_words.txt", "\n".join(invalid))
  download_file(f"{DIR_ROOT}/valid_words.txt")
  download_file(f"{DIR_ROOT}/invalid_words.txt")

# analyze_words(6)

# Генерация итоговых заданий

import math

batch_size = 3
word_repeat_count = 2

def get_manually_selected_words(batch):
  words = [w for w in read_file(f"{DIR_RESOURCES}/Отобранные слова.txt").split("\n")]
  if 0 <= batch < math.ceil(len(words) / float(batch_size)):
    return windowed(words, batch_size)[batch]
  else:
    return None

def process_word_batch(batch):
  words = get_manually_selected_words(batch)
  files_for_word = [
      generate_1_definition_4_words_question_for_single_word(w, word_repeat_count)
      for w in words
  ]
  for files in files_for_word:
    for file, output in files.items():
      save_file(file, output)
      download_file(file)

# Анализ таблицы по опросам

import math
import pandas as pd
import statistics

table = pd.read_csv(f"{DIR_RESOURCES}/Опросы для курсовой - Лист1.csv")
columns = [
    c for c in list(table.columns)
    if c.startswith("Общая оценка")
]
avg = {
    c: statistics.mean(table[c])
    for c in columns
}
lower = {
    c: avg for c, avg in avg.items()
    if 2 <= avg < 3
}
above_3 = {
    c: avg for c, avg in avg.items()
    if 3 <= avg < 4
}
above_4 = {
    c: avg for c, avg in avg.items()
    if avg >= 4
}
for c in [lower, above_3, above_4]:
  print(len(c))
  print(*c.items(), sep = "\n")

# create_tests_from_logs()
# download_tests()

input(f"Выберите режим работы ({', '.join(request_modes)}): ")

# process_word_batch(0)

# for i_batch in range(0, math.ceil(221 / float(batch_size))):
#   process_word_batch(i_batch)

# random_words = get_random_words_with_different_frequencies(
#     word_class = 6,
#     count = 10,
#     rng_seed = 2
# )

# selected_words = "\n".join([
#     str((w, c, f))
#     for w, (c, f) in random_words
# ])
# print(f"Для теста отобраны слова:\n{selected_words}\n")

# request_mode = input(f"Выберите режим работы ({', '.join(request_modes)}): ")

# generate_1_definition_4_words_question_for_single_word(1)